{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open glove and tokenize-pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print( \"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "model_glove = loadGloveModel('../glove/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_df = pd.DataFrame(model_glove).T\n",
    "\n",
    "unk_pad_df = pd.DataFrame(columns=glove_df.columns)\n",
    "unk_pad_df.loc['<PAD>'] = np.zeros(glove_df.shape[1])\n",
    "unk_pad_df.loc['<UNK>'] = glove_df.mean()\n",
    "\n",
    "glove_unk_df = pd.concat([unk_pad_df,glove_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = []\n",
    "for i in os.listdir('data/'):\n",
    "    if i.endswith('.txt'):\n",
    "        files.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['dialogue'])\n",
    "for i in files:\n",
    "    df_loop = pd.read_table('data/'+i, sep=\"\\n\", header=None)\n",
    "    df_loop = df_loop.rename(columns={0:'dialogue'})\n",
    "    df = df.append(df_loop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def delete_blank(x):\n",
    "    if x=='':\n",
    "        return None\n",
    "    else:\n",
    "        return x\n",
    "df.dialogue = df.dialogue.map(delete_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_parentheses(x):\n",
    "    return re.sub(r'\\(.*\\)|\\[.*\\]', '', x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dialogue = df.dialogue.map(lambda x: delete_parentheses(x))\n",
    "df.dialogue = df.dialogue.map(delete_blank)\n",
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punctuation(x):\n",
    "#     x = re.sub(r'\\.',' . ', x)\n",
    "#     x = re.sub(r'\\,',' , ', x)\n",
    "#     x = re.sub(r'\\!',' ! ', x)\n",
    "#     x = re.sub(r'\\?',' ? ', x)\n",
    "#     x = re.sub(r'\\:',' : ', x) \n",
    "    x = re.sub(r'\\.',' ', x)\n",
    "    x = re.sub(r'\\,',' ', x)\n",
    "    x = re.sub(r'\\!',' ', x)\n",
    "    x = re.sub(r'\\?',' ', x)\n",
    "#     x = re.sub(r'\\:',' ', x) \n",
    "    return x\n",
    "\n",
    "def delete_large_spaces(x):\n",
    "    return re.sub(r'\\s{2,}', ' ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dialogue = df.dialogue.map(lambda x: separate_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dialogue = df.dialogue.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_words(x):\n",
    "    x = re.sub(\"let's\", 'let us', x)\n",
    "    x = re.sub(\"let’s\", 'let us', x)\n",
    "    x = re.sub(\"c'mon\", 'come on', x)\n",
    "    x = re.sub(\"c’mon\", 'come on', x)\n",
    "    x = re.sub(\"there’s\", 'there is', x)\n",
    "    x = re.sub(\"there's\", 'there is', x)\n",
    "    x = re.sub(\"you're\", 'you are', x)\n",
    "    x = re.sub(\"you’re\", 'you are', x)\n",
    "    x = re.sub(\"we're\", 'we are', x)\n",
    "    x = re.sub(\"we’re\", 'we are', x)\n",
    "    x = re.sub(\"i'm\", 'i am', x)\n",
    "    x = re.sub(\"i’m\", 'i am', x)\n",
    "    x = re.sub(\"y'\", 'you', x)\n",
    "    x = re.sub(\"y’\", 'you', x)\n",
    "    x = re.sub(\"how'd\", 'how did', x)\n",
    "    x = re.sub(\"how’d\", 'how did', x)\n",
    "    x = re.sub(\"\\'ll\", ' will', x)\n",
    "    x = re.sub(\"\\’t\", ' not', x)\n",
    "    x = re.sub(\"\\'t\", ' not', x)\n",
    "    x = re.sub(\"\\'s\", '  is', x)\n",
    "    x = re.sub(\"\\’s\", '  is', x)\n",
    "    x = re.sub(\"\\'re\", '  are', x)\n",
    "    x = re.sub(\"\\’re\", '  are', x)\n",
    "    x = re.sub(\"\\'\", ' ', x)\n",
    "    x = re.sub('\\\"', ' ', x)\n",
    "    x = re.sub('-', ' ', x)\n",
    "    x = re.sub('pheebs', 'phoebe', x)\n",
    "    x = re.sub('wasn', 'was not', x)\n",
    "    x = re.sub('noo', 'no', x)\n",
    "    x = re.sub(\"didn\", 'did', x)\n",
    "    \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dialogue = df.dialogue.map(change_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_no_dialogue(x):\n",
    "    if ':' in x:\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dialogue = df.dialogue.map(delete_no_dialogue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_names(x):\n",
    "    x = re.sub(r'[a-z]{2,}:','', x)\n",
    "    return x\n",
    "\n",
    "def delete_semicol(x):\n",
    "    x = re.sub(r'\\:','', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dialogue = df.dialogue.map(delete_names)\n",
    "df.dialogue = df.dialogue.map(delete_large_spaces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna().reset_index(drop=True)\n",
    "df.dialogue = df.dialogue.map(delete_semicol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only same kind of length conversations\n",
    "\n",
    "max_len = 10\n",
    "df['answer'] = df.dialogue.shift(-1)\n",
    "df['counts_dialogue'] = df.dialogue.map(lambda x: len(x.split()))\n",
    "df = df.drop(len(df)-1)\n",
    "df['counts_answer'] = df.answer.map(lambda x: len(x.split()))\n",
    "# df = df[(df.counts_dialogue<max_len+1)&(df.counts_answer<max_len+1)]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for memory reasons, use just the vocabulary from friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "\n",
    "script_string = list(df.answer.as_matrix().flatten())\n",
    "most_common_words = pd.Series(''.join(script_string).split()).value_counts().head(top_words)\n",
    "vocab_friends = pd.Series(most_common_words.index)\n",
    "vocab_friends = pd.Series(['<PAD>', '<UNK>']).append(vocab_friends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_unk_friends_df = glove_unk_df[glove_unk_df.index.isin(vocab_friends)]\n",
    "voc_df = pd.DataFrame(glove_unk_friends_df.index, columns=['voc']).reset_index()\n",
    "voc_df.voc = voc_df.voc.str.lower()\n",
    "voc_df = voc_df.set_index('voc')\n",
    "voc_dic = voc_df.to_dict()['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "class TokenizerCustom(Tokenizer):\n",
    "    def __init__(self, voc, max_len=max_len, *args, **kwargs):\n",
    "        super(TokenizerCustom, self).__init__(*args, **kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.word_index = voc\n",
    "        self.oov_token = '<unk>'\n",
    "        self.filters = '#$%&()*+-/<=>@[\\]^_`{|}~.,'\n",
    "    def pad_string(self, x):\n",
    "        return pad_sequences(x, maxlen=self.max_len)\n",
    "    \n",
    "    def tokenize_string(self, x):\n",
    "        tok_str = self.texts_to_sequences(pd.Series(x).values)\n",
    "        return self.pad_string(tok_str)[0]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0, 1457,   90,    1,    1, 1033,  623,   65],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test unk\n",
    "tc = TokenizerCustom(voc=voc_dic, oov_token=voc_dic['<unk>'], max_len=max_len)\n",
    "tc.tokenize_string(df.answer.loc[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dic_inv = {voc_dic[x]:x for x in voc_dic}\n",
    "\n",
    "X = df.dialogue.map(lambda x: tc.tokenize_string(x))\n",
    "X = np.array(X.tolist())\n",
    "y = df.answer.map(lambda x: tc.tokenize_string(x))\n",
    "y = np.array(y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       you\n",
       "1      guys\n",
       "2       can\n",
       "3        we\n",
       "4    please\n",
       "5       not\n",
       "6     watch\n",
       "7      this\n",
       "8       all\n",
       "9     right\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y[0]).map(voc_dic_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_words_overall(y, times, myid, voc_dic_inv):\n",
    "    len_shape = len(voc_dic_inv)\n",
    "    new_index = len_shape\n",
    "    i = 0\n",
    "    row, col = np.where(y==myid)\n",
    "    for pos in range(times+1):\n",
    "        row_loop, col_loop = row[i:i+int(len(row)/times)], col[i:i+int(len(row)/times)]\n",
    "        y[row_loop, col_loop] = new_index\n",
    "        new_index += 1\n",
    "        i += int(len(row)/times)\n",
    "        voc_dic_inv[new_index] = voc_dic_inv[myid]\n",
    "\n",
    "    return y, voc_dic_inv\n",
    "\n",
    "\n",
    "\n",
    "num_col = y.shape[1]\n",
    "voc_dic_inv_copy = voc_dic_inv.copy()\n",
    "counts_ser = pd.Series(y.flatten()).value_counts()\n",
    "data_words = pd.Series(y.flatten()).value_counts().index\n",
    "min_count = pd.Series(y.flatten()).value_counts().iloc[-1]\n",
    "\n",
    "for i in range(len(data_words)):\n",
    "    word = data_words[i]\n",
    "    y, voc_dic_inv = remap_words_overall(y, int(counts_ser.loc[word]/min_count), word, voc_dic_inv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def remap_words(y, times, myid, voc_dic_inv):\n",
    "#     len_shape = len(voc_dic_inv)\n",
    "#     new_index = len_shape\n",
    "#     i = 0\n",
    "#     row = np.where(y==myid)[0]\n",
    "#     for pos in range(times):\n",
    "#         row_loop = row[i:i+int(len(row)/times)]\n",
    "#         y[row_loop] = new_index\n",
    "#         i += int(len(row)/times)\n",
    "#         voc_dic_inv[new_index] = voc_dic_inv[myid]\n",
    "#         new_index += 1\n",
    "#     return y, voc_dic_inv\n",
    "\n",
    "\n",
    "# num_col = y.shape[1]\n",
    "# dic_all_dic = {}\n",
    "# for j in range(num_col):\n",
    "#     voc_dic_inv_copy = voc_dic_inv.copy()\n",
    "#     counts_ser = pd.Series(y[:,j]).value_counts()\n",
    "#     data_words = pd.Series(y[:,j]).value_counts().index\n",
    "#     min_count = pd.Series(y[:,j]).value_counts().iloc[-1]\n",
    "#     print(min_count)\n",
    "#     for i in range(len(data_words)):\n",
    "#         word = data_words[i]\n",
    "#         y[:,j], voc_dic_inv_copy = remap_words(y[:,j], int(counts_ser.loc[word]/min_count), word, voc_dic_inv_copy)\n",
    "#     dic_all_dic[j] = voc_dic_inv_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(y[0])):\n",
    "#     print(dic_all_dic[i][y[0][i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = len(X[0])\n",
    "emb_dim = glove_unk_friends_df.shape[1]\n",
    "vocab_dim = glove_unk_friends_df.shape[0]\n",
    "# vocab_out_dim = len(voc_dic_inv)\n",
    "vocab_out_dim = y.flatten().max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5014,  8629,  7334, ...,  6904,  7846,  7809],\n",
       "       [ 1464,  1465,  1466, ...,  1471,  1472, 11938],\n",
       "       [ 9272, 11812, 10860, ..., 10965,  6579, 10797],\n",
       "       ...,\n",
       "       [ 5002,  5003,  5004, ...,  9152,  8082,  9912],\n",
       "       [ 5005,  5006,  5007, ...,  7469,  6835,  7470],\n",
       "       [ 5012,  7425,  7332, ...,  9816,  9882,  7844]], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# # one hot encode target sequence\n",
    "# def encode_output(sequences, vocab_size):\n",
    "#     ylist = list()\n",
    "#     for sequence in sequences:\n",
    "#         encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "#         ylist.append(encoded)\n",
    "#     y = np.array(ylist, dtype=np.uint8)\n",
    "# #     y = y.reshape(y.shape[0], vocab_size, y.shape[1])\n",
    "\n",
    "#     return y\n",
    "\n",
    "# y_enc = encode_output(y, vocab_out_dim)\n",
    "\n",
    "\n",
    "y_enc = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_enc = y_enc_copy.reshape(y_enc_copy.shape[0], vocab_dim, y_enc_copy.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, Dropout, RepeatVector, Flatten, Activation, Permute, Reshape\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "# inp = Input(shape=(max_seq_len,))\n",
    "# emb = Embedding(vocab_dim, emb_dim, weights=[glove_unk_friends_df], \n",
    "#                 input_length=max_seq_len, trainable=False, mask_zero=True)(inp)\n",
    "\n",
    "\n",
    "# lstm_in = LSTM(500, dropout=0.0)(emb)\n",
    "# rep_vec = RepeatVector(max_seq_len)(lstm_in)\n",
    "# rep_vec = TimeDistributed(Dense(vocab_dim, activation='softmax'))(rep_vec)\n",
    "# lstm_out = LSTM(500, dropout=0.0, return_sequences=True)(rep_vec)\n",
    "# out = TimeDistributed(Dense(vocab_dim, activation='softmax'))(lstm_out)\n",
    "\n",
    "\n",
    "# out = AttentionDecoder(150, vocab_dim)(lstm_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import multiply, BatchNormalization\n",
    "\n",
    "inp = Input(shape=(max_seq_len,))\n",
    "emb = Embedding(vocab_dim, 100, weights=[glove_unk_friends_df], \n",
    "                input_length=max_seq_len, trainable=False, mask_zero=True)(inp)\n",
    "emb = BatchNormalization()(emb)\n",
    "lstm_in = LSTM(200, dropout=0, return_sequences=True)(emb)\n",
    "lstm_in = LSTM(200, dropout=0)(lstm_in)\n",
    "\n",
    "rep_vec = RepeatVector(max_seq_len)(lstm_in)\n",
    "rep_vec = TimeDistributed(Dense(vocab_out_dim, activation='relu', W_regularizer=regularizers.l2(0)))(rep_vec)\n",
    "\n",
    "# lstm_in_2 = LSTM(400, dropout=0)(emb)\n",
    "# rep_vec_2 = RepeatVector(max_seq_len)(lstm_in)\n",
    "# rep_vec_2 = TimeDistributed(Dense(vocab_out_dim, activation='relu', W_regularizer=regularizers.l2(0.01)))(rep_vec_2)\n",
    "\n",
    "# rep_vec = multiply([rep_vec, rep_vec_2])\n",
    "\n",
    "lstm_out = LSTM(200, dropout=0, return_sequences=True)(rep_vec)\n",
    "# lstm_out = LSTM(200, dropout=0, return_sequences=True)(lstm_out)\n",
    "\n",
    "out = TimeDistributed(Dense(vocab_out_dim, activation='softmax', W_regularizer=regularizers.l2(0)))(lstm_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add( Embedding(vocab_dim, emb_dim, weights=[glove_unk_friends_df], \n",
    "#                 input_length=max_seq_len, trainable=False, mask_zero=False))\n",
    "# model.add(LSTM(200, return_sequences=True))\n",
    "# model.add(LSTM(200, return_sequences=True))\n",
    "# if True:\n",
    "#     model.add(Dropout(0.5))\n",
    "# model.add(TimeDistributed(Dense(vocab_dim)))\n",
    "# model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "earlystop = EarlyStopping(monitor='acc', min_delta=0.001, patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='acc', factor=0.2,\n",
    "                              patience=5, min_lr=0.00001, verbose=1)\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "# model = Model(inputs=inp, outputs=out)\n",
    "# model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0005), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 10, 100)           146400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10, 100)           400       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 200)           240800    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 10, 200)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 10, 12483)         2509083   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 10, 200)           10147200  \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 10, 12483)         2509083   \n",
      "=================================================================\n",
      "Total params: 15,873,766\n",
      "Trainable params: 15,727,166\n",
      "Non-trainable params: 146,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# plot_model(model, show_shapes=True, show_layer_names=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 9.1767 - acc: 0.0000e+00\n",
      "Epoch 2/5000\n",
      "986/986 [==============================] - 44s 45ms/step - loss: 9.0076 - acc: 2.0284e-04\n",
      "Epoch 3/5000\n",
      "986/986 [==============================] - 42s 43ms/step - loss: 8.8975 - acc: 1.0142e-04\n",
      "Epoch 4/5000\n",
      "986/986 [==============================] - 41s 42ms/step - loss: 8.8004 - acc: 2.0284e-04\n",
      "Epoch 5/5000\n",
      "986/986 [==============================] - 43s 44ms/step - loss: 8.7136 - acc: 1.0142e-04\n",
      "Epoch 6/5000\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 8.6305 - acc: 1.0142e-04\n",
      "Epoch 7/5000\n",
      "986/986 [==============================] - 54s 55ms/step - loss: 8.5729 - acc: 5.0710e-04\n",
      "Epoch 8/5000\n",
      "986/986 [==============================] - 47s 48ms/step - loss: 8.5014 - acc: 8.1136e-04\n",
      "Epoch 9/5000\n",
      "986/986 [==============================] - 48s 48ms/step - loss: 8.4470 - acc: 0.0011\n",
      "Epoch 10/5000\n",
      "986/986 [==============================] - 48s 49ms/step - loss: 8.3908 - acc: 0.0013\n",
      "Epoch 11/5000\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 8.3383 - acc: 0.0010\n",
      "Epoch 12/5000\n",
      "986/986 [==============================] - 54s 55ms/step - loss: 8.2764 - acc: 0.0015\n",
      "Epoch 13/5000\n",
      "986/986 [==============================] - 52s 53ms/step - loss: 8.2303 - acc: 0.0022\n",
      "Epoch 14/5000\n",
      "986/986 [==============================] - 49s 50ms/step - loss: 8.1930 - acc: 0.0026\n",
      "Epoch 15/5000\n",
      "986/986 [==============================] - 52s 53ms/step - loss: 8.1208 - acc: 0.0018\n",
      "Epoch 16/5000\n",
      "986/986 [==============================] - 50s 51ms/step - loss: 8.0744 - acc: 0.0027\n",
      "Epoch 17/5000\n",
      "986/986 [==============================] - 52s 52ms/step - loss: 8.0266 - acc: 0.0030\n",
      "Epoch 18/5000\n",
      "986/986 [==============================] - 48s 49ms/step - loss: 7.9785 - acc: 0.0043\n",
      "Epoch 19/5000\n",
      "832/986 [========================>.....] - ETA: 7s - loss: 7.9258 - acc: 0.0044"
     ]
    }
   ],
   "source": [
    "# model.fit(X, np.array(y.tolist()), epochs = 10, callbacks=[earlystop], batch_size=10)\n",
    "model.fit(X, y_enc, epochs = 5000, callbacks=[reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encoder and decoder gloabal LSTM variables with 300 units\n",
    "# LSTM_cell = LSTM(300, return_state = True)\n",
    "# LSTM_decoder = LSTM(300, return_state = True, return_sequences = True)\n",
    "# # final dense layer that uses TimeDistributed wrapper to generate 'vocab_size' softmax outputs for each time step in the decoder lstm\n",
    "# dense = TimeDistributed(Dense(vocab_size, activation = 'softmax'))\n",
    "\n",
    "# input_context = Input(shape = (maxLen, ), dtype = 'int32', name = 'input_context')\n",
    "# input_target = Input(shape = (maxLen, ), dtype = 'int32', name = 'input_target')\n",
    "\n",
    "# # pass the inputs into the embedding layer\n",
    "# input_ctx_embed = embed_layer(input_context)\n",
    "# input_tar_embed = embed_layer(input_target)\n",
    "\n",
    "# # pass the embeddings into the corresponding LSTM layers\n",
    "# encoder_lstm, context_h, context_c = LSTM_cell(input_ctx_embed)\n",
    "# # the decoder lstm uses the final states from the encoder lstm as the initial state\n",
    "# decoder_lstm, _, _ = LSTM_decoder(input_tar_embed, initial_state = [context_h, context_c],)0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(pd.Series(['<pad>','what','my','name','is','ana','what','is','your','name']).map(voc_dic).fillna(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(a.reshape(1, X.shape[1]))[0]\n",
    "result_df = pd.DataFrame(result).idxmax(axis=1, skipna=True)\n",
    "print(np.array(result_df.map(voc_dic_inv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dic_inv[419]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(result)):\n",
    "#     print(dic_all_dic[i][result_df.iloc[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "# generate a sequence of random integers\n",
    "def generate_sequence(length, n_unique):\n",
    "\treturn [randint(0, n_unique-1) for _ in range(length)]\n",
    "\n",
    "# one hot encode sequence\n",
    "def one_hot_encode(sequence, n_unique):\n",
    "\tencoding = list()\n",
    "\tfor value in sequence:\n",
    "\t\tvector = [0 for _ in range(n_unique)]\n",
    "\t\tvector[value] = 1\n",
    "\t\tencoding.append(vector)\n",
    "\treturn array(encoding)\n",
    "\n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "\treturn [argmax(vector) for vector in encoded_seq]\n",
    "\n",
    "# prepare data for the LSTM\n",
    "def get_pair(n_in, n_out, cardinality):\n",
    "\t# generate random sequence\n",
    "\tsequence_in = generate_sequence(n_in, cardinality)\n",
    "\tsequence_out = sequence_in[:n_out] + [0 for _ in range(n_in-n_out)]\n",
    "\t# one hot encode\n",
    "\tX = one_hot_encode(sequence_in, cardinality)\n",
    "\ty = one_hot_encode(sequence_out, cardinality)\n",
    "\t# reshape as 3D\n",
    "\tX = X.reshape((1, X.shape[0], X.shape[1]))\n",
    "\ty = y.reshape((1, y.shape[0], y.shape[1]))\n",
    "\treturn X,y\n",
    "\n",
    "# configure problem\n",
    "n_features = 50\n",
    "n_timesteps_in = 5\n",
    "n_timesteps_out = 2\n",
    "# define model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(150, input_shape=(n_timesteps_in, n_features)))\n",
    "# model.add(RepeatVector(n_timesteps_in))\n",
    "# model.add(LSTM(150, return_sequences=True))\n",
    "# model.add(TimeDistributed(Dense(n_features, activation='softmax')))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "inp = Input(shape=(n_timesteps_in, n_features))\n",
    "\n",
    "lstm_in = LSTM(200, dropout=0.1)(inp)\n",
    "rep_vec = RepeatVector(n_timesteps_in)(lstm_in)\n",
    "rep_vec = TimeDistributed(Dense(vocab_dim, activation='softmax', W_regularizer=regularizers.l2(0.01)))(rep_vec)\n",
    "\n",
    "lstm_in_2 = LSTM(200, dropout=0.1)(inp)\n",
    "rep_vec_2 = RepeatVector(n_timesteps_in)(lstm_in)\n",
    "rep_vec_2 = TimeDistributed(Dense(vocab_dim, activation='relu', W_regularizer=regularizers.l2(0.01)))(rep_vec_2)\n",
    "\n",
    "rep_vec = multiply([rep_vec, rep_vec_2])\n",
    "lstm_out = LSTM(200, dropout=0.1, return_sequences=True)(rep_vec)\n",
    "out = TimeDistributed(Dense(n_features, activation='softmax', W_regularizer=regularizers.l2(0.01)))(lstm_out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# train LSTM\n",
    "for epoch in range(5000):\n",
    "\t# generate new random sequence\n",
    "\tX, y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
    "\tX = X.reshape(X.shape[2],X.shape[1])\n",
    "\t# fit model for one epoch on this sequence\n",
    "\tmodel.fit(X, y, epochs=1, verbose=2)\n",
    "# evaluate LSTM\n",
    "total, correct = 100, 0\n",
    "for _ in range(total):\n",
    "\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
    "\tyhat = model.predict(X, verbose=0)\n",
    "\tif array_equal(one_hot_decode(y[0]), one_hot_decode(yhat[0])):\n",
    "\t\tcorrect += 1\n",
    "print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))\n",
    "# spot check some examples\n",
    "for _ in range(10):\n",
    "\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
    "\tyhat = model.predict(X, verbose=0)\n",
    "\tprint('Expected:', one_hot_decode(y[0]), 'Predicted', one_hot_decode(yhat[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    X,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
    "    yhat = model.predict(X, verbose=0)\n",
    "    print(X)\n",
    "    print(one_hot_decode(X[0]))\n",
    "    print('Expected:', one_hot_decode(y[0]), 'Predicted', one_hot_decode(yhat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(X[100]).map(voc_dic_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_unk_friends_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                            input_dim=None, output_dim=None,\n",
    "                            timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "class AttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='AttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states\n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
    "            \"Neural machine translation by jointly learning to align and translate.\"\n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super(AttentionDecoder, self).reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, s\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for creating the context vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the r (reset) gate\n",
    "        \"\"\"\n",
    "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_r = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_r',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for the z (update) gate\n",
    "        \"\"\"\n",
    "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_z = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_z',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the proposal\n",
    "        \"\"\"\n",
    "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_p = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_p',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for making the final prediction vector\n",
    "        \"\"\"\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "        return super(AttentionDecoder, self).call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, s0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        ytm, stm = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        _stm = K.repeat(stm, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        _Wxstm = K.dot(_stm, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.exp(et)\n",
    "        at_sum = K.sum(at, axis=1)\n",
    "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
    "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        # ~~~> calculate new hidden state\n",
    "        # first calculate the \"r\" gate:\n",
    "\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_r)\n",
    "            + K.dot(stm, self.U_r)\n",
    "            + K.dot(context, self.C_r)\n",
    "            + self.b_r)\n",
    "\n",
    "        # now calculate the \"z\" gate\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_z)\n",
    "            + K.dot(stm, self.U_z)\n",
    "            + K.dot(context, self.C_z)\n",
    "            + self.b_z)\n",
    "\n",
    "        # calculate the proposal hidden state:\n",
    "        s_tp = activations.tanh(\n",
    "            K.dot(ytm, self.W_p)\n",
    "            + K.dot((rt * stm), self.U_p)\n",
    "            + K.dot(context, self.C_p)\n",
    "            + self.b_p)\n",
    "\n",
    "        # new hidden state:\n",
    "        st = (1-zt)*stm + zt * s_tp\n",
    "\n",
    "        yt = activations.softmax(\n",
    "            K.dot(ytm, self.W_o)\n",
    "            + K.dot(stm, self.U_o)\n",
    "            + K.dot(context, self.C_o)\n",
    "            + self.b_o)\n",
    "\n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, st]\n",
    "        else:\n",
    "            return yt, [yt, st]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super(AttentionDecoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
