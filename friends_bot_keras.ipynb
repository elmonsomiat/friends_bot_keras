{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = []\n",
    "for i in os.listdir('data/'):\n",
    "    if i.endswith('.txt'):\n",
    "        files.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['dialogue'])\n",
    "for i in files:\n",
    "    df_loop = pd.read_table('data/'+i, sep=\"\\n\", header=None)\n",
    "    df_loop = df_loop.rename(columns={0:'dialogue'})\n",
    "    df = df.append(df_loop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def delete_blank(x):\n",
    "    if x=='':\n",
    "        return None\n",
    "    else:\n",
    "        return x\n",
    "df.dialogue = df.dialogue.map(delete_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_parentheses(x):\n",
    "    return re.sub(r'\\(.*\\)|\\[.*\\]', '', x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dialogue = df.dialogue.map(lambda x: delete_parentheses(x))\n",
    "df.dialogue = df.dialogue.map(delete_blank)\n",
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punctuation(x):\n",
    "#     x = re.sub(r'\\.',' . ', x)\n",
    "    x = re.sub(r'\\,',' ', x)\n",
    "#     x = re.sub(r'\\!',' ! ', x)\n",
    "#     x = re.sub(r'\\?',' ? ', x)\n",
    "#     x = re.sub(r'\\:',' : ', x) \n",
    "    x = re.sub(r'\\.',' ', x)\n",
    "    x = re.sub(r'\\,',' ', x)\n",
    "    x = re.sub(r'\\!',' ', x)\n",
    "    x = re.sub(r'\\?',' ', x)\n",
    "#     x = re.sub(r'\\:',' ', x) \n",
    "    return x\n",
    "\n",
    "def delete_large_spaces(x):\n",
    "    return re.sub(r'\\s{2,}', ' ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dialogue = df.dialogue.map(lambda x: separate_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dialogue = df.dialogue.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_words(x):\n",
    "    x = re.sub(\"let's\", 'let us', x)\n",
    "    x = re.sub(\"let’s\", 'let us', x)\n",
    "    x = re.sub(\"c'mon\", 'come on', x)\n",
    "    x = re.sub(\"c’mon\", 'come on', x)\n",
    "    x = re.sub(\"there’s\", 'there is', x)\n",
    "    x = re.sub(\"there's\", 'there is', x)\n",
    "    x = re.sub(\"you're\", 'you are', x)\n",
    "    x = re.sub(\"you’re\", 'you are', x)\n",
    "    x = re.sub(\"we're\", 'we are', x)\n",
    "    x = re.sub(\"we’re\", 'we are', x)\n",
    "    x = re.sub(\"i'm\", 'i am', x)\n",
    "    x = re.sub(\"i’m\", 'i am', x)\n",
    "    x = re.sub(\"y'\", 'you', x)\n",
    "    x = re.sub(\"y’\", 'you', x)\n",
    "    x = re.sub(\"how'd\", 'how did', x)\n",
    "    x = re.sub(\"how’d\", 'how did', x)\n",
    "    x = re.sub(\"\\'ll\", ' will', x)\n",
    "    x = re.sub(\"\\’t\", ' not', x)\n",
    "    x = re.sub(\"\\'t\", ' not', x)\n",
    "    x = re.sub(\"\\'s\", '  is', x)\n",
    "    x = re.sub(\"\\’s\", '  is', x)\n",
    "    x = re.sub(\"\\'re\", '  are', x)\n",
    "    x = re.sub(\"\\’re\", '  are', x)\n",
    "    x = re.sub(\"\\'\", ' ', x)\n",
    "    x = re.sub('\\\"', ' ', x)\n",
    "    x = re.sub('-', ' ', x)\n",
    "    x = re.sub('pheebs', 'phoebe', x)\n",
    "    x = re.sub('wasn', 'was not', x)\n",
    "    x = re.sub('noo', 'no', x)\n",
    "    x = re.sub(\"didn\", 'did', x)\n",
    "    \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dialogue = df.dialogue.map(change_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_no_dialogue(x):\n",
    "    if ':' in x:\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phoebe: oh  oh  it  is on again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>joey: you guys  can we please not watch this a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all: shhhh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>host: folks  has this ever happened to you  yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joey:  aw  there is got to be a better way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mike: and there is kevin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>joey: can we please turn this off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rachel: no way  kevin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mike: there is a revolutionary new product tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ross:  are you intrigued</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>chandler: you are flingin  flangin  right i am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mike: keep in mind  he  is never used this pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>joey:  now  i can have milk everyday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>opening credits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>chandler: well  it  is official there are no g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>janice: well  let us go to a bad one and make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>monica: perhaps  you would like me to turn lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>joey:  hey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phoebe: hey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>chandler: hey  man  what  is up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>joey: maybe you can tell me  my agent would li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>chandler: well  i will tell ya i do enjoy guil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>phoebe: yes  it was  it was him  uh huh   okay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>joey: how is it you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>phoebe: well  it was just  it was all so crazy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>joey: yep  that  is my audition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>monica: see  now this is why i keep notepads e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>phoebe: yep  and that  is why we don not invit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>janice: what is the great tragedy here  you go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>joey: well  estelle tried  you know  the casti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>ross:  where there is no fear of commitment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>chandler: do we have any    do we have any tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>joey: well  i ve never been through the tunnel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>rachel: amazingly  that makes sense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>chandler: you think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>joey: oh  yeah  go for it man  jump off the hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>chandler: yeah  joe  i assure you if i am star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>phoebe: oh  it  is your audition from this mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>rachel: sure phoebe  you know  that  is what i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>joey: come on baby  come on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>phoebe:   hi  i have phoebe buffay returning a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>rachel: very nice touch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>phoebe:  op  went through a tunnel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>rachel: unbelievable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>joey: thank you so much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>phoebe: it was really fun  i mean i ve never t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>joey: you were amazing  could you just do me t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>phoebe: oh  i don not know  i mean it was fun ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>joey: come on  please  it will be just this on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>phoebe: two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>joey: yeah  well  well really it  is three  pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>phoebe: okay  i will do it  but just these thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>joey: noo  four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>janice: so  how come you wanted to eat in toni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>chandler:  cause  i wanted to uh  give you this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>janice: ohhh  are you a puppy   contact paper ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>chandler: well  wait there is  there is more  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>janice: oh  you did not have to do this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>chandler: yes  i did  yes  i did  because  you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>janice: well  i gotta buy a vowel  because  oh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             dialogue\n",
       "0                    phoebe: oh  oh  it  is on again \n",
       "1   joey: you guys  can we please not watch this a...\n",
       "2                                         all: shhhh \n",
       "3   host: folks  has this ever happened to you  yo...\n",
       "4         joey:  aw  there is got to be a better way \n",
       "5                           mike: and there is kevin \n",
       "6                  joey: can we please turn this off \n",
       "7                              rachel: no way  kevin \n",
       "8   mike: there is a revolutionary new product tha...\n",
       "9                           ross:  are you intrigued \n",
       "10    chandler: you are flingin  flangin  right i am \n",
       "11  mike: keep in mind  he  is never used this pro...\n",
       "12              joey:  now  i can have milk everyday \n",
       "13                                    opening credits\n",
       "14  chandler: well  it  is official there are no g...\n",
       "15  janice: well  let us go to a bad one and make ...\n",
       "16  monica: perhaps  you would like me to turn lik...\n",
       "17                                        joey:  hey \n",
       "18                                       phoebe: hey \n",
       "19                   chandler: hey  man  what  is up \n",
       "20  joey: maybe you can tell me  my agent would li...\n",
       "21  chandler: well  i will tell ya i do enjoy guil...\n",
       "22  phoebe: yes  it was  it was him  uh huh   okay...\n",
       "23                               joey: how is it you \n",
       "24  phoebe: well  it was just  it was all so crazy...\n",
       "25                   joey: yep  that  is my audition \n",
       "26  monica: see  now this is why i keep notepads e...\n",
       "27  phoebe: yep  and that  is why we don not invit...\n",
       "28  janice: what is the great tragedy here  you go...\n",
       "29  joey: well  estelle tried  you know  the casti...\n",
       "..                                                ...\n",
       "70       ross:  where there is no fear of commitment \n",
       "71  chandler: do we have any    do we have any tho...\n",
       "72  joey: well  i ve never been through the tunnel...\n",
       "73               rachel: amazingly  that makes sense \n",
       "74                               chandler: you think \n",
       "75  joey: oh  yeah  go for it man  jump off the hi...\n",
       "76  chandler: yeah  joe  i assure you if i am star...\n",
       "77  phoebe: oh  it  is your audition from this mor...\n",
       "78  rachel: sure phoebe  you know  that  is what i...\n",
       "79                       joey: come on baby  come on \n",
       "80  phoebe:   hi  i have phoebe buffay returning a...\n",
       "81                           rachel: very nice touch \n",
       "82                phoebe:  op  went through a tunnel \n",
       "83                              rachel: unbelievable \n",
       "84                           joey: thank you so much \n",
       "85  phoebe: it was really fun  i mean i ve never t...\n",
       "86  joey: you were amazing  could you just do me t...\n",
       "87  phoebe: oh  i don not know  i mean it was fun ...\n",
       "88  joey: come on  please  it will be just this on...\n",
       "89                                       phoebe: two \n",
       "90  joey: yeah  well  well really it  is three  pl...\n",
       "91  phoebe: okay  i will do it  but just these thr...\n",
       "92                                   joey: noo  four \n",
       "93  janice: so  how come you wanted to eat in toni...\n",
       "94  chandler:  cause  i wanted to uh  give you this  \n",
       "95  janice: ohhh  are you a puppy   contact paper ...\n",
       "96  chandler: well  wait there is  there is more  ...\n",
       "97           janice: oh  you did not have to do this \n",
       "98  chandler: yes  i did  yes  i did  because  you...\n",
       "99  janice: well  i gotta buy a vowel  because  oh...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dialogue = df.dialogue.map(delete_no_dialogue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_names(x):\n",
    "    x = re.sub(r'[a-z]{2,}:','', x)\n",
    "    return x\n",
    "\n",
    "def delete_semicol(x):\n",
    "    x = re.sub(r'\\:','', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dialogue = df.dialogue.map(delete_names)\n",
    "df.dialogue = df.dialogue.map(delete_large_spaces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna().reset_index(drop=True)\n",
    "df.dialogue = df.dialogue.map(delete_semicol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only same kind of length conversations\n",
    "\n",
    "max_len = 8\n",
    "df['answer'] = df.dialogue.shift(-1)\n",
    "df['counts_dialogue'] = df.dialogue.map(lambda x: len(x.split()))\n",
    "df = df.drop(len(df)-1)\n",
    "df['counts_answer'] = df.answer.map(lambda x: len(x.split()))\n",
    "# df = df[(df.counts_dialogue<max_len+1)&(df.counts_answer<max_len+1)]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "shit_data = {\n",
    "\t\"conversations\": [\n",
    "\t\t[\n",
    "\t\t\t\"Good morning, how are you?\",\n",
    "\t\t\t\"I am doing well, how about you?\",\n",
    "\t\t\t\"I'm also good.\",\n",
    "\t\t\t\"That's good to hear.\",\n",
    "\t\t\t\"Yes it is.\"\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t\"Hello\",\n",
    "\t\t\t\"Hi\",\n",
    "\t\t\t\"How are you doing?\",\n",
    "\t\t\t\"I am doing well.\",\n",
    "\t\t\t\"That is good to hear\",\n",
    "\t\t\t\"Yes it is.\",\n",
    "\t\t\t\"Can I help you with anything?\",\n",
    "\t\t\t\"Yes, I have a question.\",\n",
    "\t\t\t\"What is your question?\",\n",
    "\t\t\t\"Could I borrow a cup of sugar?\",\n",
    "\t\t\t\"I'm sorry, but I don't have any.\",\n",
    "\t\t\t\"Thank you anyway\",\n",
    "\t\t\t\"No problem\"\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t\"How are you doing?\",\n",
    "\t\t\t\"I am doing well, how about you?\",\n",
    "\t\t\t\"I am also good.\",\n",
    "\t\t\t\"That's good.\"\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t\"Have you heard the news?\",\n",
    "\t\t\t\"What good news?\"\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t\"What is your favorite book?\",\n",
    "\t\t\t\"I can't read.\",\n",
    "\t\t\t\"So what's your favorite color?\",\n",
    "\t\t\t\"Blue\"\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t\"Who are you?\",\n",
    "\t\t\t\"Who? Who is but a form following the function of what\",\n",
    "\t\t\t\"What are you then?\",\n",
    "\t\t\t\"A man in a mask.\",\n",
    "\t\t\t\"I can see that.\",\n",
    "\t\t\t\"It's not your powers of observation I doubt, but merely the paradoxical nature of asking a masked man who is. But tell me, do you like music?\",\n",
    "\t\t\t\"I like seeing movies.\",\n",
    "\t\t\t\"What kind of movies do you like?\",\n",
    "\t\t\t\"Alice in Wonderland\",\n",
    "\t\t\t\"I wish I was The Mad Hatter.\",\n",
    "\t\t\t\"You're entirely bonkers. But I'll tell you a secret. All the best people are.\"\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t\"I am working on a project\",\n",
    "\t\t\t\"What are you working on?\",\n",
    "\t\t\t\"I am baking a cake.\"\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t\"The cake is a lie.\",\n",
    "\t\t\t\"No it is not. The cake is delicious.\",\n",
    "\t\t\t\"What else is delicious?\",\n",
    "\t\t\t\"Nothing\",\n",
    "\t\t\t\"Or something\",\n",
    "\t\t\t\"Tell me about your self.\",\n",
    "\t\t\t\"What do you want to know?\",\n",
    "\t\t\t\"Are you a robot?\",\n",
    "\t\t\t\"Yes I am.\",\n",
    "\t\t\t\"What is it like?\",\n",
    "\t\t\t\"What is it that you want to know?\",\n",
    "\t\t\t\"How do you work?\",\n",
    "\t\t\t\"Its complicated.\",\n",
    "\t\t\t\"Complex is better than complicated.\"\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t\"Complex is better than complicated.\",\n",
    "\t\t\t\"Simple is better than complex.\",\n",
    "\t\t\t\"In the face of ambiguity, refuse the temptation to guess.\",\n",
    "\t\t\t\"It seems your familiar with the Zen of Python\",\n",
    "\t\t\t\"I am.\",\n",
    "\t\t\t\"Do you know all of it?\",\n",
    "\t\t\t\"Beautiful is better than ugly.\",\n",
    "\t\t\t\"Explicit is better than implicit.\",\n",
    "\t\t\t\"Simple is better than complex.\",\n",
    "\t\t\t\"Complex is better than complicated.\",\n",
    "\t\t\t\"Flat is better than nested.\",\n",
    "\t\t\t\"Sparse is better than dense.\",\n",
    "\t\t\t\"Readability counts.\",\n",
    "\t\t\t\"Special cases aren't special enough to break the rules.\",\n",
    "\t\t\t\"Although practicality beats purity.\",\n",
    "\t\t\t\"Errors should never pass silently.\",\n",
    "\t\t\t\"Unless explicitly silenced.\",\n",
    "\t\t\t\"In the face of ambiguity, refuse the temptation to guess.\",\n",
    "\t\t\t\"There should be one-- and preferably only one --obvious way to do it.\",\n",
    "\t\t\t\"Although that way may not be obvious at first unless you're Dutch.\",\n",
    "\t\t\t\"Now is better than never.\",\n",
    "\t\t\t\"Although never is often better than right now.\",\n",
    "\t\t\t\"If the implementation is hard to explain, it's a bad idea.\",\n",
    "\t\t\t\"If the implementation is easy to explain, it may be a good idea.\",\n",
    "\t\t\t\"Namespaces are one honking great idea. Let's do more of those!\",\n",
    "\t\t\t\"I agree.\"\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t\"Are you a programmer?\",\n",
    "\t\t\t\"I am a programmer\",\n",
    "\t\t\t\"What languages do you like to use?\",\n",
    "\t\t\t\"I use Python, Java and C++ quite often.\",\n",
    "\t\t\t\"I use Python quite a bit myself.\",\n",
    "\t\t\t\"I'm not incredibly fond of Java.\",\n",
    "\t\t\t\"What annoys you?\",\n",
    "\t\t\t\"It has many inconsistencies.\"\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t\"What does YOLO mean?\",\n",
    "\t\t\t\"It means you only live once. Where did you hear that?\",\n",
    "\t\t\t\"I heard somebody say it.\"\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t\"Did I ever live?\",\n",
    "\t\t\t\"It depends how you define life\",\n",
    "\t\t\t\"Life is the condition that distinguishes organisms from inorganic matter, including the capacity for growth, reproduction, functional activity, and continual change preceding death.\",\n",
    "\t\t\t\"Is that a definition or an opinion?\"\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t\"Can I ask you a question?\",\n",
    "\t\t\t\"Go ahead and ask.\"\n",
    "\t\t]\n",
    "\t]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = shit_data['conversations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "for i in range(len(cor)):\n",
    "    for j in range(len(cor[i])):\n",
    "        if j<len(cor[i])-1:\n",
    "            x_data.append(cor[i][j]);\n",
    "            y_data.append(cor[i][j+1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['dialogue','answer'])\n",
    "df.dialogue = x_data\n",
    "df.answer = y_data\n",
    "df.dialogue = df.dialogue.str.lower()\n",
    "df.answer = df.answer.str.lower()\n",
    "df.dialogue = df.dialogue.map(separate_punctuation)\n",
    "df.answer = df.answer.map(separate_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open glove and tokenize-pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print( \"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "model_glove = loadGloveModel('../glove/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_df = pd.DataFrame(model_glove).T\n",
    "\n",
    "unk_pad_df = pd.DataFrame(columns=glove_df.columns)\n",
    "unk_pad_df.loc['<PAD>'] = np.zeros(glove_df.shape[1])\n",
    "unk_pad_df.loc['<UNK>'] = glove_df.mean()\n",
    "\n",
    "glove_unk_df = pd.concat([unk_pad_df,glove_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for memory reasons, use just the vocabulary from friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 2000\n",
    "\n",
    "script_string = list(df.answer.as_matrix().flatten())\n",
    "most_common_words = pd.Series(''.join(script_string).split()).value_counts().head(top_words)\n",
    "vocab_friends = pd.Series(most_common_words.index)\n",
    "vocab_friends = pd.Series(['<PAD>', '<UNK>']).append(vocab_friends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_unk_friends_df = glove_unk_df[glove_unk_df.index.isin(vocab_friends)]\n",
    "voc_df = pd.DataFrame(glove_unk_friends_df.index, columns=['voc']).reset_index()\n",
    "voc_df.voc = voc_df.voc.str.lower()\n",
    "voc_df = voc_df.set_index('voc')\n",
    "voc_dic = voc_df.to_dict()['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "class TokenizerCustom(Tokenizer):\n",
    "    def __init__(self, voc, max_len=max_len, *args, **kwargs):\n",
    "        super(TokenizerCustom, self).__init__(*args, **kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.word_index = voc\n",
    "        self.oov_token = '<unk>'\n",
    "        self.filters = '#$%&()*+-/<=>@[\\]^_`{|}~.,?!'\n",
    "    def pad_string(self, x):\n",
    "        return pad_sequences(x, maxlen=self.max_len)\n",
    "    \n",
    "    def tokenize_string(self, x):\n",
    "        tok_str = self.texts_to_sequences(pd.Series(x).values)\n",
    "        return self.pad_string(tok_str)[0]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,  36,  94,  91, 205, 201,  17], dtype=int32)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test unk\n",
    "tc = TokenizerCustom(voc=voc_dic, oov_token=voc_dic['<unk>'], max_len=max_len)\n",
    "tc.tokenize_string(df.answer.loc[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am doing well  how about you '"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dic_inv = {voc_dic[x]:x for x in voc_dic}\n",
    "\n",
    "X = df.dialogue.map(lambda x: tc.tokenize_string(x))\n",
    "X = np.array(X.tolist())\n",
    "y = df.answer.map(lambda x: tc.tokenize_string(x))\n",
    "y = np.array(y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <pad>\n",
       "1        i\n",
       "2       am\n",
       "3    doing\n",
       "4     well\n",
       "5      how\n",
       "6    about\n",
       "7      you\n",
       "dtype: object"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y[0]).map(voc_dic_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_words_overall(y, times, myid, voc_dic_inv):\n",
    "    len_shape = len(voc_dic_inv)\n",
    "    new_index = len_shape\n",
    "    i = 0\n",
    "    row, col = np.where(y==myid)\n",
    "    for pos in range(times+1):\n",
    "        row_loop, col_loop = row[i:i+int(len(row)/times)], col[i:i+int(len(row)/times)]\n",
    "        y[row_loop, col_loop] = new_index\n",
    "        new_index += 1\n",
    "        i += int(len(row)/times)\n",
    "        voc_dic_inv[new_index] = voc_dic_inv[myid]\n",
    "\n",
    "    return y, voc_dic_inv\n",
    "\n",
    "\n",
    "\n",
    "num_col = y.shape[1]\n",
    "voc_dic_inv_copy = voc_dic_inv.copy()\n",
    "counts_ser = pd.Series(y.flatten()).value_counts()\n",
    "data_words = pd.Series(y.flatten()).value_counts().index\n",
    "min_count = pd.Series(y.flatten()).value_counts().iloc[-1]\n",
    "\n",
    "for i in range(len(data_words)):\n",
    "    word = data_words[i]\n",
    "    y, voc_dic_inv = remap_words_overall(y, int(counts_ser.loc[word]/min_count), word, voc_dic_inv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def remap_words(y, times, myid, voc_dic_inv):\n",
    "#     len_shape = len(voc_dic_inv)\n",
    "#     new_index = len_shape\n",
    "#     i = 0\n",
    "#     row = np.where(y==myid)[0]\n",
    "#     for pos in range(times):\n",
    "#         row_loop = row[i:i+int(len(row)/times)]\n",
    "#         y[row_loop] = new_index\n",
    "#         i += int(len(row)/times)\n",
    "#         voc_dic_inv[new_index] = voc_dic_inv[myid]\n",
    "#         new_index += 1\n",
    "#     return y, voc_dic_inv\n",
    "\n",
    "\n",
    "# num_col = y.shape[1]\n",
    "# dic_all_dic = {}\n",
    "# for j in range(num_col):\n",
    "#     voc_dic_inv_copy = voc_dic_inv.copy()\n",
    "#     counts_ser = pd.Series(y[:,j]).value_counts()\n",
    "#     data_words = pd.Series(y[:,j]).value_counts().index\n",
    "#     min_count = pd.Series(y[:,j]).value_counts().iloc[-1]\n",
    "#     print(min_count)\n",
    "#     for i in range(len(data_words)):\n",
    "#         word = data_words[i]\n",
    "#         y[:,j], voc_dic_inv_copy = remap_words(y[:,j], int(counts_ser.loc[word]/min_count), word, voc_dic_inv_copy)\n",
    "#     dic_all_dic[j] = voc_dic_inv_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(y[0])):\n",
    "#     print(dic_all_dic[i][y[0][i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = len(X[0])\n",
    "emb_dim = glove_unk_friends_df.shape[1]\n",
    "vocab_dim = glove_unk_friends_df.shape[0]\n",
    "# vocab_out_dim = len(voc_dic_inv)\n",
    "vocab_out_dim = y.flatten().max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = np.array(ylist, dtype=np.uint8)\n",
    "#     y = y.reshape(y.shape[0], vocab_size, y.shape[1])\n",
    "\n",
    "    return y\n",
    "\n",
    "y_enc = encode_output(y, vocab_out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_enc_copy = y_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_enc = y_enc_copy.reshape(y_enc_copy.shape[0], vocab_dim, y_enc_copy.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, Dropout, RepeatVector, Flatten, Activation, Permute, Reshape\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "# inp = Input(shape=(max_seq_len,))\n",
    "# emb = Embedding(vocab_dim, emb_dim, weights=[glove_unk_friends_df], \n",
    "#                 input_length=max_seq_len, trainable=False, mask_zero=True)(inp)\n",
    "\n",
    "\n",
    "# lstm_in = LSTM(500, dropout=0.0)(emb)\n",
    "# rep_vec = RepeatVector(max_seq_len)(lstm_in)\n",
    "# rep_vec = TimeDistributed(Dense(vocab_dim, activation='softmax'))(rep_vec)\n",
    "# lstm_out = LSTM(500, dropout=0.0, return_sequences=True)(rep_vec)\n",
    "# out = TimeDistributed(Dense(vocab_dim, activation='softmax'))(lstm_out)\n",
    "\n",
    "\n",
    "# out = AttentionDecoder(150, vocab_dim)(lstm_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import multiply\n",
    "\n",
    "inp = Input(shape=(max_seq_len,))\n",
    "emb = Embedding(vocab_dim, 100, weights=[glove_unk_friends_df], \n",
    "                input_length=max_seq_len, trainable=False, mask_zero=True)(inp)\n",
    "\n",
    "lstm_in = LSTM(200, dropout=0)(emb)\n",
    "rep_vec = RepeatVector(max_seq_len)(lstm_in)\n",
    "rep_vec = TimeDistributed(Dense(vocab_out_dim, activation='relu', W_regularizer=regularizers.l2(0)))(rep_vec)\n",
    "\n",
    "# lstm_in_2 = LSTM(400, dropout=0)(emb)\n",
    "# rep_vec_2 = RepeatVector(max_seq_len)(lstm_in)\n",
    "# rep_vec_2 = TimeDistributed(Dense(vocab_out_dim, activation='softmax', W_regularizer=regularizers.l2(0.01)))(rep_vec_2)\n",
    "\n",
    "# rep_vec = multiply([rep_vec, rep_vec_2])\n",
    "\n",
    "lstm_out = LSTM(200, dropout=0, return_sequences=True)(rep_vec)\n",
    "out = TimeDistributed(Dense(vocab_out_dim, activation='softmax', W_regularizer=regularizers.l2(0)))(lstm_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add( Embedding(vocab_dim, emb_dim, weights=[glove_unk_friends_df], \n",
    "#                 input_length=max_seq_len, trainable=False, mask_zero=False))\n",
    "# model.add(LSTM(200, return_sequences=True))\n",
    "# model.add(LSTM(200, return_sequences=True))\n",
    "# if True:\n",
    "#     model.add(Dropout(0.5))\n",
    "# model.add(TimeDistributed(Dense(vocab_dim)))\n",
    "# model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "earlystop = EarlyStopping(monitor='acc', min_delta=0.001, patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='acc', factor=0.2,\n",
    "                              patience=2, min_lr=0.000001, verbose=1)\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "# model = Model(inputs=inp, outputs=out)\n",
    "# model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.005), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 8, 100)            20800     \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "repeat_vector_13 (RepeatVect (None, 8, 200)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 8, 1075)           216075    \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 8, 200)            1020800   \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 8, 1075)           216075    \n",
      "=================================================================\n",
      "Total params: 1,714,550\n",
      "Trainable params: 1,693,750\n",
      "Non-trainable params: 20,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# plot_model(model, show_shapes=True, show_layer_names=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "86/86 [==============================] - 6s 69ms/step - loss: 3.7730 - acc: 0.3256\n",
      "Epoch 2/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 4.0148 - acc: 0.1003\n",
      "Epoch 3/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 3.4827 - acc: 0.2209\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 4/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 2.9912 - acc: 0.3169\n",
      "Epoch 5/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 2.7124 - acc: 0.3692\n",
      "Epoch 6/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 2.4850 - acc: 0.4012\n",
      "Epoch 7/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 2.2892 - acc: 0.4680\n",
      "Epoch 8/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 2.1589 - acc: 0.4884\n",
      "Epoch 9/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 2.0475 - acc: 0.5174\n",
      "Epoch 10/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.9494 - acc: 0.5654\n",
      "Epoch 11/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.8684 - acc: 0.5741\n",
      "Epoch 12/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.7889 - acc: 0.6134\n",
      "Epoch 13/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.7253 - acc: 0.6337\n",
      "Epoch 14/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.6708 - acc: 0.6439\n",
      "Epoch 15/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.6204 - acc: 0.6846\n",
      "Epoch 16/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.5765 - acc: 0.7108\n",
      "Epoch 17/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.5360 - acc: 0.7398\n",
      "Epoch 18/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.4999 - acc: 0.7573\n",
      "Epoch 19/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.4666 - acc: 0.7762\n",
      "Epoch 20/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.4358 - acc: 0.7718\n",
      "Epoch 21/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.4073 - acc: 0.7849\n",
      "Epoch 22/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.3819 - acc: 0.8009\n",
      "Epoch 23/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.3561 - acc: 0.7936\n",
      "Epoch 24/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.3340 - acc: 0.8038\n",
      "Epoch 25/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.3129 - acc: 0.7936\n",
      "Epoch 26/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.2938 - acc: 0.8081\n",
      "Epoch 27/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.2741 - acc: 0.8038\n",
      "Epoch 28/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.2554 - acc: 0.8358\n",
      "Epoch 29/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.2383 - acc: 0.8169\n",
      "Epoch 30/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.2229 - acc: 0.8561\n",
      "Epoch 31/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.2079 - acc: 0.8299\n",
      "Epoch 32/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1942 - acc: 0.8372\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0001999999862164259.\n",
      "Epoch 33/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1822 - acc: 0.8067\n",
      "Epoch 34/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1745 - acc: 0.8532\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.9999996079131965e-05.\n",
      "Epoch 35/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1729 - acc: 0.8721\n",
      "Epoch 36/1000\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.1709 - acc: 0.8706\n",
      "Epoch 37/1000\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 1.1690 - acc: 0.8532\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 7.99999907030724e-06.\n",
      "Epoch 38/1000\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 1.1680 - acc: 0.8459\n",
      "Epoch 39/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1678 - acc: 0.8459\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-06.\n",
      "Epoch 40/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1677 - acc: 0.8459\n",
      "Epoch 41/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1677 - acc: 0.8459\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 42/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1676 - acc: 0.8459\n",
      "Epoch 43/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1676 - acc: 0.8459\n",
      "Epoch 44/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1676 - acc: 0.8459\n",
      "Epoch 45/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1676 - acc: 0.8459\n",
      "Epoch 46/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1675 - acc: 0.8459\n",
      "Epoch 47/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1675 - acc: 0.8459\n",
      "Epoch 48/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1675 - acc: 0.8459\n",
      "Epoch 49/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1674 - acc: 0.8459\n",
      "Epoch 50/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1674 - acc: 0.8459\n",
      "Epoch 51/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1674 - acc: 0.8459\n",
      "Epoch 52/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1674 - acc: 0.8459\n",
      "Epoch 53/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1673 - acc: 0.8459\n",
      "Epoch 54/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1673 - acc: 0.8474\n",
      "Epoch 55/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1673 - acc: 0.8474\n",
      "Epoch 56/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1673 - acc: 0.8474\n",
      "Epoch 57/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1672 - acc: 0.8474\n",
      "Epoch 58/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1672 - acc: 0.8474\n",
      "Epoch 59/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1672 - acc: 0.8474\n",
      "Epoch 60/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1672 - acc: 0.8474\n",
      "Epoch 61/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1671 - acc: 0.8474\n",
      "Epoch 62/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1671 - acc: 0.8474\n",
      "Epoch 63/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1671 - acc: 0.8474\n",
      "Epoch 64/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1670 - acc: 0.8474\n",
      "Epoch 65/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1670 - acc: 0.8474\n",
      "Epoch 66/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1670 - acc: 0.8474\n",
      "Epoch 67/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1670 - acc: 0.8474\n",
      "Epoch 68/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1669 - acc: 0.8474\n",
      "Epoch 69/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1669 - acc: 0.8474\n",
      "Epoch 70/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1669 - acc: 0.8474\n",
      "Epoch 71/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1669 - acc: 0.8474\n",
      "Epoch 72/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1669 - acc: 0.8474\n",
      "Epoch 73/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1668 - acc: 0.8474\n",
      "Epoch 74/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1668 - acc: 0.8474\n",
      "Epoch 75/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1668 - acc: 0.8474\n",
      "Epoch 76/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1668 - acc: 0.8474\n",
      "Epoch 77/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1667 - acc: 0.8474\n",
      "Epoch 78/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1667 - acc: 0.8474\n",
      "Epoch 79/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1667 - acc: 0.8474\n",
      "Epoch 80/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1667 - acc: 0.8488\n",
      "Epoch 81/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1666 - acc: 0.8488\n",
      "Epoch 82/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1666 - acc: 0.8488\n",
      "Epoch 83/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1666 - acc: 0.8488\n",
      "Epoch 84/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1666 - acc: 0.8488\n",
      "Epoch 85/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1665 - acc: 0.8488\n",
      "Epoch 86/1000\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.1665 - acc: 0.8488\n",
      "Epoch 87/1000\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.1665 - acc: 0.8488\n",
      "Epoch 88/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1665 - acc: 0.8488\n",
      "Epoch 89/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1664 - acc: 0.8488\n",
      "Epoch 90/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1664 - acc: 0.8488\n",
      "Epoch 91/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1664 - acc: 0.8488\n",
      "Epoch 92/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1664 - acc: 0.8488\n",
      "Epoch 93/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1663 - acc: 0.8488\n",
      "Epoch 94/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1663 - acc: 0.8488\n",
      "Epoch 95/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1663 - acc: 0.8488\n",
      "Epoch 96/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1663 - acc: 0.8488\n",
      "Epoch 97/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1663 - acc: 0.8488\n",
      "Epoch 98/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1662 - acc: 0.8488\n",
      "Epoch 99/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1662 - acc: 0.8488\n",
      "Epoch 100/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1662 - acc: 0.8488\n",
      "Epoch 101/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1662 - acc: 0.8503\n",
      "Epoch 102/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1661 - acc: 0.8503\n",
      "Epoch 103/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1661 - acc: 0.8503\n",
      "Epoch 104/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1661 - acc: 0.8503\n",
      "Epoch 105/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1661 - acc: 0.8488\n",
      "Epoch 106/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1660 - acc: 0.8488\n",
      "Epoch 107/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1660 - acc: 0.8488\n",
      "Epoch 108/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1660 - acc: 0.8503\n",
      "Epoch 109/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1660 - acc: 0.8503\n",
      "Epoch 110/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1659 - acc: 0.8488\n",
      "Epoch 111/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1659 - acc: 0.8488\n",
      "Epoch 112/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1659 - acc: 0.8503\n",
      "Epoch 113/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1659 - acc: 0.8503\n",
      "Epoch 114/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1658 - acc: 0.8503\n",
      "Epoch 115/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1658 - acc: 0.8503\n",
      "Epoch 116/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1658 - acc: 0.8503\n",
      "Epoch 117/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1658 - acc: 0.8503\n",
      "Epoch 118/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1657 - acc: 0.8503\n",
      "Epoch 119/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1657 - acc: 0.8503\n",
      "Epoch 120/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1657 - acc: 0.8503\n",
      "Epoch 121/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1657 - acc: 0.8503\n",
      "Epoch 122/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1657 - acc: 0.8503\n",
      "Epoch 123/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1656 - acc: 0.8503\n",
      "Epoch 124/1000\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.1656 - acc: 0.8503\n",
      "Epoch 125/1000\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.1656 - acc: 0.8503\n",
      "Epoch 126/1000\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 1.1656 - acc: 0.8503\n",
      "Epoch 127/1000\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 1.1655 - acc: 0.8503\n",
      "Epoch 128/1000\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.1655 - acc: 0.8503\n",
      "Epoch 129/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1655 - acc: 0.8503\n",
      "Epoch 130/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1655 - acc: 0.8503\n",
      "Epoch 131/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1654 - acc: 0.8503\n",
      "Epoch 132/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1654 - acc: 0.8503\n",
      "Epoch 133/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1654 - acc: 0.8503\n",
      "Epoch 134/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1654 - acc: 0.8503\n",
      "Epoch 135/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1653 - acc: 0.8503\n",
      "Epoch 136/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1653 - acc: 0.8503\n",
      "Epoch 137/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1653 - acc: 0.8503\n",
      "Epoch 138/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1653 - acc: 0.8517\n",
      "Epoch 139/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1652 - acc: 0.8517\n",
      "Epoch 140/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1652 - acc: 0.8517\n",
      "Epoch 141/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1652 - acc: 0.8517\n",
      "Epoch 142/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1652 - acc: 0.8517\n",
      "Epoch 143/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1651 - acc: 0.8517\n",
      "Epoch 144/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1651 - acc: 0.8517\n",
      "Epoch 145/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1651 - acc: 0.8517\n",
      "Epoch 146/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1651 - acc: 0.8517\n",
      "Epoch 147/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1650 - acc: 0.8517\n",
      "Epoch 148/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1650 - acc: 0.8517\n",
      "Epoch 149/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1650 - acc: 0.8517\n",
      "Epoch 150/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1650 - acc: 0.8517\n",
      "Epoch 151/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1649 - acc: 0.8517\n",
      "Epoch 152/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1649 - acc: 0.8517\n",
      "Epoch 153/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1649 - acc: 0.8517\n",
      "Epoch 154/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1649 - acc: 0.8517\n",
      "Epoch 155/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1648 - acc: 0.8517\n",
      "Epoch 156/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1648 - acc: 0.8517\n",
      "Epoch 157/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1648 - acc: 0.8517\n",
      "Epoch 158/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1648 - acc: 0.8517\n",
      "Epoch 159/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1647 - acc: 0.8517\n",
      "Epoch 160/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1647 - acc: 0.8517\n",
      "Epoch 161/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1647 - acc: 0.8517\n",
      "Epoch 162/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1647 - acc: 0.8517\n",
      "Epoch 163/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1646 - acc: 0.8517\n",
      "Epoch 164/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1646 - acc: 0.8517\n",
      "Epoch 165/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1646 - acc: 0.8503\n",
      "Epoch 166/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1646 - acc: 0.8503\n",
      "Epoch 167/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1645 - acc: 0.8503\n",
      "Epoch 168/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1645 - acc: 0.8503\n",
      "Epoch 169/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1645 - acc: 0.8503\n",
      "Epoch 170/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1645 - acc: 0.8503\n",
      "Epoch 171/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1644 - acc: 0.8503\n",
      "Epoch 172/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1644 - acc: 0.8503\n",
      "Epoch 173/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1644 - acc: 0.8503\n",
      "Epoch 174/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1644 - acc: 0.8503\n",
      "Epoch 175/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1643 - acc: 0.8503\n",
      "Epoch 176/1000\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.1643 - acc: 0.8503\n",
      "Epoch 177/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1643 - acc: 0.8503\n",
      "Epoch 178/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1643 - acc: 0.8503\n",
      "Epoch 179/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1642 - acc: 0.8503\n",
      "Epoch 180/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1642 - acc: 0.8503\n",
      "Epoch 181/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1642 - acc: 0.8503\n",
      "Epoch 182/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1642 - acc: 0.8503\n",
      "Epoch 183/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1641 - acc: 0.8503\n",
      "Epoch 184/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1641 - acc: 0.8503\n",
      "Epoch 185/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1641 - acc: 0.8503\n",
      "Epoch 186/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1641 - acc: 0.8503\n",
      "Epoch 187/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1640 - acc: 0.8503\n",
      "Epoch 188/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1640 - acc: 0.8503\n",
      "Epoch 189/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1640 - acc: 0.8503\n",
      "Epoch 190/1000\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1640 - acc: 0.8503\n",
      "Epoch 191/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1639 - acc: 0.8503\n",
      "Epoch 192/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1639 - acc: 0.8503\n",
      "Epoch 193/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1639 - acc: 0.8503\n",
      "Epoch 194/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1639 - acc: 0.8503\n",
      "Epoch 195/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1638 - acc: 0.8503\n",
      "Epoch 196/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1638 - acc: 0.8503\n",
      "Epoch 197/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1638 - acc: 0.8503\n",
      "Epoch 198/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1638 - acc: 0.8503\n",
      "Epoch 199/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1637 - acc: 0.8503\n",
      "Epoch 200/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1637 - acc: 0.8503\n",
      "Epoch 201/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1637 - acc: 0.8503\n",
      "Epoch 202/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1637 - acc: 0.8503\n",
      "Epoch 203/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1636 - acc: 0.8503\n",
      "Epoch 204/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1636 - acc: 0.8503\n",
      "Epoch 205/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1636 - acc: 0.8503\n",
      "Epoch 206/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1636 - acc: 0.8503\n",
      "Epoch 207/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1635 - acc: 0.8503\n",
      "Epoch 208/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1635 - acc: 0.8503\n",
      "Epoch 209/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1635 - acc: 0.8503\n",
      "Epoch 210/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1634 - acc: 0.8503\n",
      "Epoch 211/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1634 - acc: 0.8503\n",
      "Epoch 212/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1634 - acc: 0.8503\n",
      "Epoch 213/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1634 - acc: 0.8503\n",
      "Epoch 214/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1633 - acc: 0.8503\n",
      "Epoch 215/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1633 - acc: 0.8503\n",
      "Epoch 216/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1633 - acc: 0.8503\n",
      "Epoch 217/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1633 - acc: 0.8503\n",
      "Epoch 218/1000\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 1.1632 - acc: 0.8503\n",
      "Epoch 219/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1632 - acc: 0.8503\n",
      "Epoch 220/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1632 - acc: 0.8503\n",
      "Epoch 221/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1632 - acc: 0.8503\n",
      "Epoch 222/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1631 - acc: 0.8503\n",
      "Epoch 223/1000\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 1.1631 - acc: 0.8503\n",
      "Epoch 224/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1631 - acc: 0.8503\n",
      "Epoch 225/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1631 - acc: 0.8503\n",
      "Epoch 226/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1630 - acc: 0.8503\n",
      "Epoch 227/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1630 - acc: 0.8503\n",
      "Epoch 228/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1630 - acc: 0.8503\n",
      "Epoch 229/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1630 - acc: 0.8503\n",
      "Epoch 230/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1629 - acc: 0.8503\n",
      "Epoch 231/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1629 - acc: 0.8503\n",
      "Epoch 232/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1629 - acc: 0.8503\n",
      "Epoch 233/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1629 - acc: 0.8503\n",
      "Epoch 234/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1628 - acc: 0.8503\n",
      "Epoch 235/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1628 - acc: 0.8503\n",
      "Epoch 236/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1628 - acc: 0.8503\n",
      "Epoch 237/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1627 - acc: 0.8503\n",
      "Epoch 238/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1627 - acc: 0.8503\n",
      "Epoch 239/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1627 - acc: 0.8503\n",
      "Epoch 240/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1627 - acc: 0.8503\n",
      "Epoch 241/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1626 - acc: 0.8503\n",
      "Epoch 242/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1626 - acc: 0.8503\n",
      "Epoch 243/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1626 - acc: 0.8503\n",
      "Epoch 244/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1626 - acc: 0.8503\n",
      "Epoch 245/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1625 - acc: 0.8503\n",
      "Epoch 246/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1625 - acc: 0.8503\n",
      "Epoch 247/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1625 - acc: 0.8503\n",
      "Epoch 248/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1625 - acc: 0.8503\n",
      "Epoch 249/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1624 - acc: 0.8503\n",
      "Epoch 250/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1624 - acc: 0.8503\n",
      "Epoch 251/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1624 - acc: 0.8503\n",
      "Epoch 252/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1624 - acc: 0.8503\n",
      "Epoch 253/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1623 - acc: 0.8503\n",
      "Epoch 254/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1623 - acc: 0.8503\n",
      "Epoch 255/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1623 - acc: 0.8503\n",
      "Epoch 256/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1623 - acc: 0.8503\n",
      "Epoch 257/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1622 - acc: 0.8503\n",
      "Epoch 258/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1622 - acc: 0.8503\n",
      "Epoch 259/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1622 - acc: 0.8503\n",
      "Epoch 260/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1621 - acc: 0.8503\n",
      "Epoch 261/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1621 - acc: 0.8503\n",
      "Epoch 262/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1621 - acc: 0.8503\n",
      "Epoch 263/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1621 - acc: 0.8503\n",
      "Epoch 264/1000\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 1.1620 - acc: 0.8503\n",
      "Epoch 265/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1620 - acc: 0.8503\n",
      "Epoch 266/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1620 - acc: 0.8503\n",
      "Epoch 267/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1620 - acc: 0.8503\n",
      "Epoch 268/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1619 - acc: 0.8503\n",
      "Epoch 269/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1619 - acc: 0.8503\n",
      "Epoch 270/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1619 - acc: 0.8503\n",
      "Epoch 271/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1619 - acc: 0.8503\n",
      "Epoch 272/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1618 - acc: 0.8503\n",
      "Epoch 273/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1618 - acc: 0.8503\n",
      "Epoch 274/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1618 - acc: 0.8503\n",
      "Epoch 275/1000\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 1.1617 - acc: 0.8503\n",
      "Epoch 276/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1617 - acc: 0.8503\n",
      "Epoch 277/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1617 - acc: 0.8503\n",
      "Epoch 278/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1617 - acc: 0.8503\n",
      "Epoch 279/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1616 - acc: 0.8503\n",
      "Epoch 280/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1616 - acc: 0.8503\n",
      "Epoch 281/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1616 - acc: 0.8503\n",
      "Epoch 282/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1616 - acc: 0.8503\n",
      "Epoch 283/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1615 - acc: 0.8503\n",
      "Epoch 284/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1615 - acc: 0.8503\n",
      "Epoch 285/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1615 - acc: 0.8503\n",
      "Epoch 286/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1615 - acc: 0.8503\n",
      "Epoch 287/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1614 - acc: 0.8503\n",
      "Epoch 288/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1614 - acc: 0.8503\n",
      "Epoch 289/1000\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 1.1614 - acc: 0.8503\n",
      "Epoch 290/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1613 - acc: 0.8503\n",
      "Epoch 291/1000\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 1.1613 - acc: 0.8503\n",
      "Epoch 292/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1613 - acc: 0.8503\n",
      "Epoch 293/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1613 - acc: 0.8503\n",
      "Epoch 294/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1612 - acc: 0.8503\n",
      "Epoch 295/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1612 - acc: 0.8503\n",
      "Epoch 296/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1612 - acc: 0.8503\n",
      "Epoch 297/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1612 - acc: 0.8503\n",
      "Epoch 298/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1611 - acc: 0.8503\n",
      "Epoch 299/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1611 - acc: 0.8503\n",
      "Epoch 300/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1611 - acc: 0.8503\n",
      "Epoch 301/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1610 - acc: 0.8503\n",
      "Epoch 302/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1610 - acc: 0.8503\n",
      "Epoch 303/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1610 - acc: 0.8503\n",
      "Epoch 304/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1610 - acc: 0.8503\n",
      "Epoch 305/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1609 - acc: 0.8503\n",
      "Epoch 306/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1609 - acc: 0.8503\n",
      "Epoch 307/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1609 - acc: 0.8503\n",
      "Epoch 308/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1609 - acc: 0.8503\n",
      "Epoch 309/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1608 - acc: 0.8503\n",
      "Epoch 310/1000\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1608 - acc: 0.8503\n",
      "Epoch 311/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1608 - acc: 0.8503\n",
      "Epoch 312/1000\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.1608 - acc: 0.8503\n",
      "Epoch 313/1000\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.1607 - acc: 0.8503\n",
      "Epoch 314/1000\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 1.1607 - acc: 0.8517\n",
      "Epoch 315/1000\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 1.1607 - acc: 0.8517\n",
      "Epoch 316/1000\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1606 - acc: 0.8517\n",
      "Epoch 317/1000\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.1606 - acc: 0.8517\n",
      "Epoch 318/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1606 - acc: 0.8517\n",
      "Epoch 319/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1606 - acc: 0.8517\n",
      "Epoch 320/1000\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.1605 - acc: 0.8517\n",
      "Epoch 321/1000\n",
      "64/86 [=====================>........] - ETA: 0s - loss: 1.1233 - acc: 0.8652"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-360-fc4da9b11ea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model.fit(X, np.array(y.tolist()), epochs = 10, callbacks=[earlystop], batch_size=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/kaggle/friends_bot/env/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/kaggle/friends_bot/env/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/friends_bot/env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/friends_bot/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/friends_bot/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/friends_bot/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/friends_bot/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/friends_bot/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/friends_bot/env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model.fit(X, np.array(y.tolist()), epochs = 10, callbacks=[earlystop], batch_size=10)\n",
    "model.fit(X, y_enc, epochs = 1000, callbacks=[reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encoder and decoder gloabal LSTM variables with 300 units\n",
    "# LSTM_cell = LSTM(300, return_state = True)\n",
    "# LSTM_decoder = LSTM(300, return_state = True, return_sequences = True)\n",
    "# # final dense layer that uses TimeDistributed wrapper to generate 'vocab_size' softmax outputs for each time step in the decoder lstm\n",
    "# dense = TimeDistributed(Dense(vocab_size, activation = 'softmax'))\n",
    "\n",
    "# input_context = Input(shape = (maxLen, ), dtype = 'int32', name = 'input_context')\n",
    "# input_target = Input(shape = (maxLen, ), dtype = 'int32', name = 'input_target')\n",
    "\n",
    "# # pass the inputs into the embedding layer\n",
    "# input_ctx_embed = embed_layer(input_context)\n",
    "# input_tar_embed = embed_layer(input_target)\n",
    "\n",
    "# # pass the embeddings into the corresponding LSTM layers\n",
    "# encoder_lstm, context_h, context_c = LSTM_cell(input_ctx_embed)\n",
    "# # the decoder lstm uses the final states from the encoder lstm as the initial state\n",
    "# decoder_lstm, _, _ = LSTM_decoder(input_tar_embed, initial_state = [context_h, context_c],)0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' 'face' 'face' 'right']\n"
     ]
    }
   ],
   "source": [
    "test_out = pd.Series(['<pad>','what','do','you','want','to','do','man']).map(voc_dic)\n",
    "result = model.predict(np.array(test_out).reshape(1, X.shape[1]))[0]\n",
    "result_df = pd.DataFrame(result).idxmax(axis=1, skipna=True)\n",
    "print(np.array(result_df.map(voc_dic_inv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' 'the' 'the']\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(X[17].reshape(1, X.shape[1]))[0]\n",
    "result_df = pd.DataFrame(result).idxmax(axis=1, skipna=True)\n",
    "print(np.array(result_df.map(voc_dic_inv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(result)):\n",
    "#     print(dic_all_dic[i][result_df.iloc[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "# generate a sequence of random integers\n",
    "def generate_sequence(length, n_unique):\n",
    "\treturn [randint(0, n_unique-1) for _ in range(length)]\n",
    "\n",
    "# one hot encode sequence\n",
    "def one_hot_encode(sequence, n_unique):\n",
    "\tencoding = list()\n",
    "\tfor value in sequence:\n",
    "\t\tvector = [0 for _ in range(n_unique)]\n",
    "\t\tvector[value] = 1\n",
    "\t\tencoding.append(vector)\n",
    "\treturn array(encoding)\n",
    "\n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "\treturn [argmax(vector) for vector in encoded_seq]\n",
    "\n",
    "# prepare data for the LSTM\n",
    "def get_pair(n_in, n_out, cardinality):\n",
    "\t# generate random sequence\n",
    "\tsequence_in = generate_sequence(n_in, cardinality)\n",
    "\tsequence_out = sequence_in[:n_out] + [0 for _ in range(n_in-n_out)]\n",
    "\t# one hot encode\n",
    "\tX = one_hot_encode(sequence_in, cardinality)\n",
    "\ty = one_hot_encode(sequence_out, cardinality)\n",
    "\t# reshape as 3D\n",
    "\tX = X.reshape((1, X.shape[0], X.shape[1]))\n",
    "\ty = y.reshape((1, y.shape[0], y.shape[1]))\n",
    "\treturn X,y\n",
    "\n",
    "# configure problem\n",
    "n_features = 50\n",
    "n_timesteps_in = 5\n",
    "n_timesteps_out = 2\n",
    "# define model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(150, input_shape=(n_timesteps_in, n_features)))\n",
    "# model.add(RepeatVector(n_timesteps_in))\n",
    "# model.add(LSTM(150, return_sequences=True))\n",
    "# model.add(TimeDistributed(Dense(n_features, activation='softmax')))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "inp = Input(shape=(n_timesteps_in, n_features))\n",
    "\n",
    "lstm_in = LSTM(200, dropout=0.1)(inp)\n",
    "rep_vec = RepeatVector(n_timesteps_in)(lstm_in)\n",
    "rep_vec = TimeDistributed(Dense(vocab_dim, activation='softmax', W_regularizer=regularizers.l2(0.01)))(rep_vec)\n",
    "\n",
    "lstm_in_2 = LSTM(200, dropout=0.1)(inp)\n",
    "rep_vec_2 = RepeatVector(n_timesteps_in)(lstm_in)\n",
    "rep_vec_2 = TimeDistributed(Dense(vocab_dim, activation='relu', W_regularizer=regularizers.l2(0.01)))(rep_vec_2)\n",
    "\n",
    "rep_vec = multiply([rep_vec, rep_vec_2])\n",
    "lstm_out = LSTM(200, dropout=0.1, return_sequences=True)(rep_vec)\n",
    "out = TimeDistributed(Dense(n_features, activation='softmax', W_regularizer=regularizers.l2(0.01)))(lstm_out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# train LSTM\n",
    "for epoch in range(5000):\n",
    "\t# generate new random sequence\n",
    "\tX, y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
    "\tX = X.reshape(X.shape[2],X.shape[1])\n",
    "\t# fit model for one epoch on this sequence\n",
    "\tmodel.fit(X, y, epochs=1, verbose=2)\n",
    "# evaluate LSTM\n",
    "total, correct = 100, 0\n",
    "for _ in range(total):\n",
    "\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
    "\tyhat = model.predict(X, verbose=0)\n",
    "\tif array_equal(one_hot_decode(y[0]), one_hot_decode(yhat[0])):\n",
    "\t\tcorrect += 1\n",
    "print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))\n",
    "# spot check some examples\n",
    "for _ in range(10):\n",
    "\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
    "\tyhat = model.predict(X, verbose=0)\n",
    "\tprint('Expected:', one_hot_decode(y[0]), 'Predicted', one_hot_decode(yhat[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    X,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
    "    yhat = model.predict(X, verbose=0)\n",
    "    print(X)\n",
    "    print(one_hot_decode(X[0]))\n",
    "    print('Expected:', one_hot_decode(y[0]), 'Predicted', one_hot_decode(yhat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(X[100]).map(voc_dic_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_unk_friends_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                            input_dim=None, output_dim=None,\n",
    "                            timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "class AttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='AttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states\n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
    "            \"Neural machine translation by jointly learning to align and translate.\"\n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super(AttentionDecoder, self).reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, s\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for creating the context vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the r (reset) gate\n",
    "        \"\"\"\n",
    "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_r = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_r',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for the z (update) gate\n",
    "        \"\"\"\n",
    "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_z = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_z',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the proposal\n",
    "        \"\"\"\n",
    "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_p = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_p',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for making the final prediction vector\n",
    "        \"\"\"\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "        return super(AttentionDecoder, self).call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, s0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        ytm, stm = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        _stm = K.repeat(stm, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        _Wxstm = K.dot(_stm, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.exp(et)\n",
    "        at_sum = K.sum(at, axis=1)\n",
    "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
    "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        # ~~~> calculate new hidden state\n",
    "        # first calculate the \"r\" gate:\n",
    "\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_r)\n",
    "            + K.dot(stm, self.U_r)\n",
    "            + K.dot(context, self.C_r)\n",
    "            + self.b_r)\n",
    "\n",
    "        # now calculate the \"z\" gate\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_z)\n",
    "            + K.dot(stm, self.U_z)\n",
    "            + K.dot(context, self.C_z)\n",
    "            + self.b_z)\n",
    "\n",
    "        # calculate the proposal hidden state:\n",
    "        s_tp = activations.tanh(\n",
    "            K.dot(ytm, self.W_p)\n",
    "            + K.dot((rt * stm), self.U_p)\n",
    "            + K.dot(context, self.C_p)\n",
    "            + self.b_p)\n",
    "\n",
    "        # new hidden state:\n",
    "        st = (1-zt)*stm + zt * s_tp\n",
    "\n",
    "        yt = activations.softmax(\n",
    "            K.dot(ytm, self.W_o)\n",
    "            + K.dot(stm, self.U_o)\n",
    "            + K.dot(context, self.C_o)\n",
    "            + self.b_o)\n",
    "\n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, st]\n",
    "        else:\n",
    "            return yt, [yt, st]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super(AttentionDecoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
